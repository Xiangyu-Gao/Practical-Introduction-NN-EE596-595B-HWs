{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extract MNIST data</h2>\n",
    "<p style=\"font-size:20px\">You can change the option of one_hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#you can print out the label for 1st figure\n",
    "print(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = mnist.train.num_examples #55,000\n",
    "num_validation = mnist.validation.num_examples #5000\n",
    "num_test = mnist.test.num_examples #10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(num_train)\n",
    "print(num_validation)\n",
    "print(num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Try different parameter combinations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_set = [0.001]\n",
    "num_steps_set = [500]\n",
    "batch_size_set = [128]\n",
    "n_hidden_1_set = [512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, Accuracy= 0.133\n",
      "step 10000, Accuracy= 0.695\n",
      "step 20000, Accuracy= 0.797\n",
      "step 30000, Accuracy= 0.688\n",
      "step 40000, Accuracy= 0.781\n",
      "step 50000, Accuracy= 0.812\n",
      "step 60000, Accuracy= 0.883\n",
      "step 70000, Accuracy= 0.883\n",
      "step 80000, Accuracy= 0.852\n",
      "step 90000, Accuracy= 0.891\n",
      "step 100000, Accuracy= 0.875\n",
      "step 110000, Accuracy= 0.883\n",
      "step 120000, Accuracy= 0.906\n",
      "step 130000, Accuracy= 0.945\n",
      "step 140000, Accuracy= 0.906\n",
      "lr:0.001000,num_steps:150000,batch_size=128,n_hidden_1=512,Testing ACcuracy:0.909100\n"
     ]
    }
   ],
   "source": [
    "#fixed network parameters\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "n_hidden_2 = 256\n",
    "dropout_prob = 0.6\n",
    "regularizer_rate = 0.1\n",
    "epochs = 15\n",
    "#varied network parameters\n",
    "for lr in lr_set:\n",
    "    for num_steps in num_steps_set:\n",
    "        for batch_size in batch_size_set:\n",
    "            for n_hidden_1 in n_hidden_1_set:\n",
    "                    \n",
    "                #Define placeholder and Variables\n",
    "                tf.reset_default_graph()\n",
    "                #tf graph input\n",
    "                X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "                Y = tf.placeholder(tf.float32,[None,num_classes],name='Y')\n",
    "                ## for dropout layer\n",
    "                keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "                #Layers weight & bias\n",
    "                weights = {\n",
    "                    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "                    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "                    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes]),name='Wout')\n",
    "                }\n",
    "\n",
    "                biases = {\n",
    "                    'b1': tf.Variable(tf.random_normal(shape=[n_hidden_1]),name='b1'),\n",
    "                    'b2': tf.Variable(tf.random_normal(shape=[n_hidden_2]),name='b2'),\n",
    "                    'bout': tf.Variable(tf.random_normal(shape=[num_classes]),name='bout')\n",
    "                }\n",
    "\n",
    "                #define nueral network\n",
    "                def neural_net(x):\n",
    "                    layer_1_out = tf.nn.relu(tf.add(tf.matmul(x,weights['W1']),biases['b1']))\n",
    "                    layer_1_out_drop = tf.nn.dropout(layer_1_out, keep_prob)\n",
    "                    layer_2_out = tf.nn.relu(tf.add(tf.matmul(layer_1_out_drop,weights['W2']),biases['b2']))\n",
    "                    layer_2_out_drop = tf.nn.dropout(layer_2_out, keep_prob)\n",
    "                    out = tf.sigmoid(tf.add(tf.matmul(layer_2_out_drop,weights['Wout']),biases['bout']))\n",
    "                    return out     \n",
    "\n",
    "                #define cost function and accuracy\n",
    "                #predicted labels\n",
    "                logits = neural_net(X)\n",
    "\n",
    "                #define loss\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y)) \\\n",
    "                + regularizer_rate*(tf.reduce_sum(tf.square(biases['b1'])) + tf.reduce_sum(tf.square(biases['b2'])))\n",
    "                \n",
    "                #define optimizer\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "                train_op = optimizer.minimize(loss)\n",
    "\n",
    "                #compare the predicted labels with true labels\n",
    "                correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "                #compute the accuracy by taking average\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "                #Initialize the variables\n",
    "                init = tf.global_variables_initializer()\n",
    "\n",
    "                #execute training\n",
    "                with tf.Session() as sess:\n",
    "                    sess.run(init)\n",
    "                    for epoch in range(epochs)\n",
    "                        for i in range(num_steps):\n",
    "                            #fetch batch\n",
    "                            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                            #run optimization\n",
    "                            sess.run(train_op, feed_dict={X:batch_x, Y:batch_y,keep_prob:dropout_prob})\n",
    "                        \n",
    "                        acc = sess.run(accuracy,feed_dict={X:batch_x, Y:batch_y,keep_prob:1})\n",
    "                        print(\"step \"+str(i)+\", Training ccuracy= {:.3f}\".format(acc))\n",
    "                        print(\"lr:%f,num_steps:%d,batch_size=%d,n_hidden_1=%d,Testing ACcuracy:%f\" \\\n",
    "                              % (lr, num_steps, batch_size, n_hidden_1, \\\n",
    "                                 sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels,keep_prob:1})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Add dropoff, regularization </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:0, Train loss: 46.37 Train acc: 0.961, Test acc:0.936\n",
      "Epoch:1, Train loss: 26.64 Train acc: 0.977, Test acc:0.954\n",
      "Epoch:2, Train loss: 15.60 Train acc: 0.984, Test acc:0.961\n",
      "Epoch:3, Train loss: 9.32 Train acc: 0.992, Test acc:0.968\n",
      "Epoch:4, Train loss: 5.80 Train acc: 0.938, Test acc:0.970\n",
      "Epoch:5, Train loss: 3.80 Train acc: 0.938, Test acc:0.973\n",
      "Epoch:6, Train loss: 2.68 Train acc: 0.984, Test acc:0.976\n",
      "Epoch:7, Train loss: 2.08 Train acc: 1.000, Test acc:0.975\n",
      "Epoch:8, Train loss: 1.77 Train acc: 1.000, Test acc:0.978\n",
      "Epoch:9, Train loss: 1.61 Train acc: 1.000, Test acc:0.978\n",
      "Epoch:10, Train loss: 1.53 Train acc: 0.992, Test acc:0.979\n",
      "Epoch:11, Train loss: 1.49 Train acc: 1.000, Test acc:0.979\n",
      "Epoch:12, Train loss: 1.48 Train acc: 0.977, Test acc:0.978\n",
      "Epoch:13, Train loss: 1.47 Train acc: 0.992, Test acc:0.980\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "## Loading MNIST dataset from keras\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "## Importing required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)\n",
    "\n",
    "num_train = mnist.train.num_examples #55,000\n",
    "num_validation = mnist.validation.num_examples #5000\n",
    "num_test = mnist.test.num_examples #10,000\n",
    "\n",
    "s = tf.InteractiveSession()\n",
    "## Defining various initialization parameters for 784-512-256-10 MLP model\n",
    "num_classes = 10\n",
    "num_features = 784\n",
    "num_output = 10\n",
    "num_layers_0 = 512\n",
    "num_layers_1 = 256\n",
    "starter_learning_rate = 0.001\n",
    "regularizer_rate = 0.1\n",
    "\n",
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32',shape =(None,num_features),name=\"input_X\")\n",
    "input_y = tf.placeholder('float32',shape = (None,num_classes),name='input_Y')\n",
    "## for dropout layer\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "## Weights initialized by random normal function with std_dev = 1/sqrt(number of input features)\n",
    "weights_0 = tf.Variable(tf.random_normal([num_features,num_layers_0], stddev=(1/tf.sqrt(float(num_features)))))\n",
    "bias_0 = tf.Variable(tf.random_normal([num_layers_0]))\n",
    "weights_1 = tf.Variable(tf.random_normal([num_layers_0,num_layers_1], stddev=(1/tf.sqrt(float(num_layers_0)))))\n",
    "bias_1 = tf.Variable(tf.random_normal([num_layers_1]))\n",
    "weights_2 = tf.Variable(tf.random_normal([num_layers_1,num_output], stddev=(1/tf.sqrt(float(num_layers_1)))))\n",
    "bias_2 = tf.Variable(tf.random_normal([num_output]))\n",
    "\n",
    "## Initializing weigths and biases\n",
    "hidden_output_0 = tf.nn.relu(tf.matmul(input_X,weights_0)+bias_0)\n",
    "hidden_output_0_0 = tf.nn.dropout(hidden_output_0, keep_prob)\n",
    "hidden_output_1 = tf.nn.relu(tf.matmul(hidden_output_0_0,weights_1)+bias_1)\n",
    "hidden_output_1_1 = tf.nn.dropout(hidden_output_1, keep_prob)\n",
    "predicted_y = tf.sigmoid(tf.matmul(hidden_output_1_1,weights_2) + bias_2)\n",
    "\n",
    "## Defining the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predicted_y,labels=input_y)) \\\n",
    "        + regularizer_rate*(tf.reduce_sum(tf.square(bias_0)) + tf.reduce_sum(tf.square(bias_1)))\n",
    "## Variable learning rate\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, 0, 5, 0.85, staircase=True)\n",
    "## Adam optimzer for finding the right weight\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss,var_list=[weights_0,weights_1,weights_2,\n",
    "                                                                         bias_0,bias_1,bias_2])\n",
    "correct_prediction = tf.equal(tf.argmax(input_y,1), tf.argmax(predicted_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "## Training parameters\n",
    "batch_size = 128\n",
    "epochs=14\n",
    "dropout_prob = 0.6\n",
    "training_accuracy = []\n",
    "training_loss = []\n",
    "testing_accuracy = []\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(epochs):    \n",
    "    arr = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    for index in range(0,50000,batch_size):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        s.run(optimizer, feed_dict={input_X:batch_x, input_y:batch_y, keep_prob:dropout_prob})\n",
    "    training_accuracy.append(s.run(accuracy, feed_dict= {input_X:batch_x, \n",
    "                                                         input_y:batch_y,keep_prob:1}))\n",
    "    training_loss.append(s.run(loss, {input_X: batch_x, \n",
    "                                      input_y: batch_y,keep_prob:1}))\n",
    "    \n",
    "    # Evaluation of model\n",
    "    testing_accuracy.append(accuracy_score(mnist.test.labels.argmax(1), \n",
    "                           s.run(predicted_y, {input_X: mnist.test.images, keep_prob:1}).argmax(1)))\n",
    "    print(\"Epoch:{0}, Train loss: {1:.2f} Train acc: {2:.3f}, Test acc:{3:.3f}\".format(epoch,training_loss[epoch], \\\n",
    "                                                                                       training_accuracy[epoch],testing_accuracy[epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Summary</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried different parameters based on the initial code. The results are shown in the table below. However, the testing accuracy doesn't change too much. This is may because of the overfitting and activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline index & lr & steps &batch\\,size &size\\,of\\,hidden\\,layer1 & accuracy\\\\\\hline\n",
    "  1 &0.01 & 500&128&300&0.884500 \\\\\\hline\n",
    "  2 &0.01 & 1000&128&300&0.867900 \\\\\\hline\n",
    "  3 &0.01 & 500&64 & 300&0.865000\\\\\\hline\n",
    "  4 &0.01 & 500&128&400&0.863400 \\\\\\hline\n",
    "  4 &0.001 & 500&128&300&0.839400 \\\\\\hline\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I modified this neural network by adding the drop off layer after each hidden layer and adding L2 regularization to the loss. The activation of each hidden layer is Relu, and activation of the final output is sigmoid. The testing accuracy of this network is 98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows that adding dropoff, regularization term, changing the activation function contributed the most to improvement in performance and other parameters did not change it too much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
