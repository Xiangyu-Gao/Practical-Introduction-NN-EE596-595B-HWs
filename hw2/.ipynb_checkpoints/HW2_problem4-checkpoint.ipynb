{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import time\n",
    "from glob import glob\n",
    "from scipy import misc\n",
    "import os, cv2, random\n",
    "import pandas as pd\n",
    "from AlexNetHelper import AlexNetHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load training, validation, testing set from your preprocessed files</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count, CHANNELS, ROWS, COLS), dtype=np.uint8)\n",
    "\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)\n",
    "        data[i] = image.T\n",
    "        if i%250 == 0: print('Processed {} of {}'.format(i, count))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat.0.jpg', 'cat.1.jpg', 'cat.10.jpg', 'cat.100.jpg', 'cat.1000.jpg']\n",
      "Processed 0 of 25000\n",
      "Processed 250 of 25000\n",
      "Processed 500 of 25000\n",
      "Processed 750 of 25000\n",
      "Processed 1000 of 25000\n",
      "Processed 1250 of 25000\n",
      "Processed 1500 of 25000\n",
      "Processed 1750 of 25000\n",
      "Processed 2000 of 25000\n",
      "Processed 2250 of 25000\n",
      "Processed 2500 of 25000\n",
      "Processed 2750 of 25000\n",
      "Processed 3000 of 25000\n",
      "Processed 3250 of 25000\n",
      "Processed 3500 of 25000\n",
      "Processed 3750 of 25000\n",
      "Processed 4000 of 25000\n",
      "Processed 4250 of 25000\n",
      "Processed 4500 of 25000\n",
      "Processed 4750 of 25000\n",
      "Processed 5000 of 25000\n",
      "Processed 5250 of 25000\n",
      "Processed 5500 of 25000\n",
      "Processed 5750 of 25000\n",
      "Processed 6000 of 25000\n",
      "Processed 6250 of 25000\n",
      "Processed 6500 of 25000\n",
      "Processed 6750 of 25000\n",
      "Processed 7000 of 25000\n",
      "Processed 7250 of 25000\n",
      "Processed 7500 of 25000\n",
      "Processed 7750 of 25000\n",
      "Processed 8000 of 25000\n",
      "Processed 8250 of 25000\n",
      "Processed 8500 of 25000\n",
      "Processed 8750 of 25000\n",
      "Processed 9000 of 25000\n",
      "Processed 9250 of 25000\n",
      "Processed 9500 of 25000\n",
      "Processed 9750 of 25000\n",
      "Processed 10000 of 25000\n",
      "Processed 10250 of 25000\n",
      "Processed 10500 of 25000\n",
      "Processed 10750 of 25000\n",
      "Processed 11000 of 25000\n",
      "Processed 11250 of 25000\n",
      "Processed 11500 of 25000\n",
      "Processed 11750 of 25000\n",
      "Processed 12000 of 25000\n",
      "Processed 12250 of 25000\n",
      "Processed 12500 of 25000\n",
      "Processed 12750 of 25000\n",
      "Processed 13000 of 25000\n",
      "Processed 13250 of 25000\n",
      "Processed 13500 of 25000\n",
      "Processed 13750 of 25000\n",
      "Processed 14000 of 25000\n",
      "Processed 14250 of 25000\n",
      "Processed 14500 of 25000\n",
      "Processed 14750 of 25000\n",
      "Processed 15000 of 25000\n",
      "Processed 15250 of 25000\n",
      "Processed 15500 of 25000\n",
      "Processed 15750 of 25000\n",
      "Processed 16000 of 25000\n",
      "Processed 16250 of 25000\n",
      "Processed 16500 of 25000\n",
      "Processed 16750 of 25000\n",
      "Processed 17000 of 25000\n",
      "Processed 17250 of 25000\n",
      "Processed 17500 of 25000\n",
      "Processed 17750 of 25000\n",
      "Processed 18000 of 25000\n",
      "Processed 18250 of 25000\n",
      "Processed 18500 of 25000\n",
      "Processed 18750 of 25000\n",
      "Processed 19000 of 25000\n",
      "Processed 19250 of 25000\n",
      "Processed 19500 of 25000\n",
      "Processed 19750 of 25000\n",
      "Processed 20000 of 25000\n",
      "Processed 20250 of 25000\n",
      "Processed 20500 of 25000\n",
      "Processed 20750 of 25000\n",
      "Processed 21000 of 25000\n",
      "Processed 21250 of 25000\n",
      "Processed 21500 of 25000\n",
      "Processed 21750 of 25000\n",
      "Processed 22000 of 25000\n",
      "Processed 22250 of 25000\n",
      "Processed 22500 of 25000\n",
      "Processed 22750 of 25000\n",
      "Processed 23000 of 25000\n",
      "Processed 23250 of 25000\n",
      "Processed 23500 of 25000\n",
      "Processed 23750 of 25000\n",
      "Processed 24000 of 25000\n",
      "Processed 24250 of 25000\n",
      "Processed 24500 of 25000\n",
      "Processed 24750 of 25000\n",
      "Processed 0 of 12500\n",
      "Processed 250 of 12500\n",
      "Processed 500 of 12500\n",
      "Processed 750 of 12500\n",
      "Processed 1000 of 12500\n",
      "Processed 1250 of 12500\n",
      "Processed 1500 of 12500\n",
      "Processed 1750 of 12500\n",
      "Processed 2000 of 12500\n",
      "Processed 2250 of 12500\n",
      "Processed 2500 of 12500\n",
      "Processed 2750 of 12500\n",
      "Processed 3000 of 12500\n",
      "Processed 3250 of 12500\n",
      "Processed 3500 of 12500\n",
      "Processed 3750 of 12500\n",
      "Processed 4000 of 12500\n",
      "Processed 4250 of 12500\n",
      "Processed 4500 of 12500\n",
      "Processed 4750 of 12500\n",
      "Processed 5000 of 12500\n",
      "Processed 5250 of 12500\n",
      "Processed 5500 of 12500\n",
      "Processed 5750 of 12500\n",
      "Processed 6000 of 12500\n",
      "Processed 6250 of 12500\n",
      "Processed 6500 of 12500\n",
      "Processed 6750 of 12500\n",
      "Processed 7000 of 12500\n",
      "Processed 7250 of 12500\n",
      "Processed 7500 of 12500\n",
      "Processed 7750 of 12500\n",
      "Processed 8000 of 12500\n",
      "Processed 8250 of 12500\n",
      "Processed 8500 of 12500\n",
      "Processed 8750 of 12500\n",
      "Processed 9000 of 12500\n",
      "Processed 9250 of 12500\n",
      "Processed 9500 of 12500\n",
      "Processed 9750 of 12500\n",
      "Processed 10000 of 12500\n",
      "Processed 10250 of 12500\n",
      "Processed 10500 of 12500\n",
      "Processed 10750 of 12500\n",
      "Processed 11000 of 12500\n",
      "Processed 11250 of 12500\n",
      "Processed 11500 of 12500\n",
      "Processed 11750 of 12500\n",
      "Processed 12000 of 12500\n",
      "Processed 12250 of 12500\n",
      "Train shape: (25000, 227, 227, 3)\n",
      "Test shape: (12500, 227, 227, 3)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'C:/Users/Xiangyu Gao/Downloads/train/'\n",
    "TEST_DIR = 'C:/Users/Xiangyu Gao/Downloads/test1/'\n",
    "\n",
    "ROWS = 227\n",
    "COLS = 227\n",
    "CHANNELS = 3\n",
    "print(os.listdir(TRAIN_DIR)[:5])\n",
    "train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\n",
    "train_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "train_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "\n",
    "test_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "\n",
    "# slice datasets for memory efficiency on Kaggle Kernels, delete if using full dataset\n",
    "train_images = train_dogs[:] + train_cats[:]\n",
    "random.shuffle(train_images)\n",
    "test_images =  test_images[:]\n",
    "\n",
    "train_labels = []\n",
    "\n",
    "for i in train_images:\n",
    "    if 'dog' in i.split('/')[-1]:\n",
    "        train_labels.append([1,0])\n",
    "    else:\n",
    "        train_labels.append([0,1])\n",
    "\n",
    "#sns.countplot(labels).set_title('Cats and Dogs')\n",
    "train_labels = np.array(train_labels)\n",
    "# \n",
    "\n",
    "train_images = prep_data(train_images)\n",
    "test_images = prep_data(test_images)\n",
    "train_images = np.swapaxes(train_images, 1,3)\n",
    "test_images = np.swapaxes(test_images, 1,3)\n",
    "\n",
    "print(\"Train shape: {}\".format(train_images.shape))\n",
    "print(\"Test shape: {}\".format(test_images.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_images = train_images[20000:25000]\n",
    "vali_labels = train_labels[20000:25000]\n",
    "train_images = train_images[0:20000]\n",
    "train_labels = train_labels[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n",
      "(5000, 2)\n",
      "(20000, 227, 227, 3)\n",
      "(5000, 227, 227, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_labels))\n",
    "print(np.shape(vali_labels))\n",
    "print(np.shape(train_images))\n",
    "print(np.shape(vali_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "#train_images, train_labels, valid_images = get_data()\n",
    "num_classes = train_labels.shape[1]\n",
    "print(np.shape(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define hyperparameter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 227, 227, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# images = tf.placeholder(tf.float32, shape=[None, *train[0].shape])\n",
    "# labels = tf.placeholder(tf.float32, shape=[None, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AlexNet</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images, num_classes):\n",
    "        \"\"\"Builds AlexNet Graph\n",
    "\n",
    "        Args:\n",
    "            images: train/test images\n",
    "            num_classes: number of classes\n",
    "        Returns:\n",
    "            final_node: last node in the graph\n",
    "        \"\"\"\n",
    "\n",
    "        # Layer 1\n",
    "        with tf.name_scope(\"conv1\") as scope:\n",
    "            kernel = AlexNetHelper.instintiate_weights('W1', [11, 11, 3, 96])\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 4, 4, 1], padding='SAME')\n",
    "            biases = AlexNetHelper.instintiate_bias('b1', [96])\n",
    "            conv1 = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope)\n",
    "\n",
    "        with tf.name_scope('batch_norm1') as scope:\n",
    "            mean, var = tf.nn.moments(conv1, [0, 1, 2])\n",
    "            batch_norm1 = tf.nn.batch_normalization(conv1, mean, var, 0, 1, 0, name=scope)\n",
    "\n",
    "        with tf.name_scope('pool1') as scope:\n",
    "            pool1 = tf.nn.max_pool(batch_norm1, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME', name=scope)\n",
    "\n",
    "        # Layer 2\n",
    "        with tf.name_scope(\"conv2\") as scope:\n",
    "            kernel = AlexNetHelper.instintiate_weights('W2', [5, 5, 96, 256])\n",
    "            conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = AlexNetHelper.instintiate_bias('b2', [256])\n",
    "            conv2 = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope)\n",
    "\n",
    "        with tf.name_scope('batch_norm2') as scope:\n",
    "            mean, var = tf.nn.moments(conv2, [0, 1, 2])\n",
    "            batch_norm2 = tf.nn.batch_normalization(conv2, mean, var, 0, 1, 0, name=scope)\n",
    "\n",
    "        with tf.name_scope('pool2') as scope:\n",
    "            pool2 = tf.nn.max_pool(batch_norm2, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME', name=scope)\n",
    "\n",
    "        # Layer 3\n",
    "        with tf.name_scope(\"conv3\") as scope:\n",
    "            kernel = AlexNetHelper.instintiate_weights('W3', [3, 3, 256, 384])\n",
    "            conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = AlexNetHelper.instintiate_bias('b3', [384])\n",
    "            conv3 = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope)\n",
    "\n",
    "        with tf.name_scope('batch_norm3') as scope:\n",
    "            mean, var = tf.nn.moments(conv3, [0, 1, 2])\n",
    "            batch_norm3 = tf.nn.batch_normalization(conv3, mean, var, 0, 1, 0, name=scope)\n",
    "\n",
    "        # Layer 4\n",
    "        with tf.name_scope(\"conv4\") as scope:\n",
    "            kernel = AlexNetHelper.instintiate_weights('W4', [3, 3, 384, 384])\n",
    "            conv = tf.nn.conv2d(batch_norm3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = AlexNetHelper.instintiate_bias('b4', [384])\n",
    "            conv4 = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope)\n",
    "\n",
    "        with tf.name_scope('batch_norm4') as scope:\n",
    "            mean, var = tf.nn.moments(conv4, [0, 1, 2])\n",
    "            batch_norm4 = tf.nn.batch_normalization(conv4, mean, var, 0, 1, 0, name=scope)\n",
    "\n",
    "        # Layer 5\n",
    "        with tf.name_scope(\"conv5\") as scope:\n",
    "            kernel = AlexNetHelper.instintiate_weights('W5', [3, 3, 384, 256])\n",
    "            conv = tf.nn.conv2d(batch_norm4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = AlexNetHelper.instintiate_bias('b5', [256])\n",
    "            conv5 = tf.nn.relu(tf.nn.bias_add(conv, biases), name=scope)\n",
    "\n",
    "        with tf.name_scope('batch_norm5') as scope:\n",
    "            mean, var = tf.nn.moments(conv5, [0, 1, 2])\n",
    "            batch_norm5 = tf.nn.batch_normalization(conv5, mean, var, 0, 1, 0, name=scope)\n",
    "\n",
    "        with tf.name_scope('pool5') as scope:\n",
    "            pool5 = tf.nn.max_pool(batch_norm5, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME', name=scope)\n",
    "\n",
    "        # Fully Connected 6\n",
    "        with tf.name_scope('FC6') as scope:\n",
    "            pool5_flat = tf.contrib.layers.flatten(pool5)\n",
    "            fc6 = tf.layers.dense(pool5_flat, units=4096, activation=tf.nn.relu, name=scope)\n",
    "            # fc6_dropout = tf.layers.dropout(fc6, 0.5)\n",
    "\n",
    "        with tf.name_scope('FC7') as scope:\n",
    "            fc7 = tf.layers.dense(fc6, units=4096, activation=tf.nn.relu, name=scope)\n",
    "            # fc7_dropout = tf.layers.dropout(fc7, 0.5)\n",
    "\n",
    "        with tf.name_scope('classes') as scope:\n",
    "            predictions = tf.layers.dense(fc7, units=num_classes, activation=tf.nn.softmax, name=scope)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost and Optimization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training and validation</h1>\n",
    "<h2>Train your model only 10 epochs</h2>\n",
    "<p style=\"font-size:20px\">1. Print out training accuracy and validation accuracy each training epoch</p>\n",
    "<p style=\"font-size:20px\">2. Print out training time each training epoch</p>\n",
    "<p style=\"font-size:20px\">3. Your goal is to reach 85% validation accuracy in 10 training epochs. If you reach that, you can perform testing, print out your test accuracy. Plot out the ten images with title that contains the probability of the labeled class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 227, 227, 3)\n",
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Runtime for AlexNet\n",
    ":param\n",
    "train_data: training data\n",
    "num_classes: num_classes\n",
    ":return: None\n",
    "\"\"\"\n",
    "print(np.shape(train_images))\n",
    "print(np.shape(train_labels))\n",
    "tf.reset_default_graph()\n",
    "num_classes = train_labels.shape[1]\n",
    "\n",
    "images = tf.placeholder(tf.float32, shape=[None, 227,227,3])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "predictions = inference(images, num_classes)\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.l2_loss(tf.square(predictions - labels)))\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = predictions, labels = ph_labels))\n",
    "learner = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n",
    "optimizer = learner.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for ep in range(epochs):\n",
    "        l_total = 0\n",
    "        acc = 0\n",
    "        start_time = time.time()\n",
    "        for i in range(0, len(train_images), batch_size):\n",
    "            sess.run(optimizer, feed_dict={images: train_images[i: batch_size + i,:], \\\n",
    "                                            labels: train_labels[i: batch_size + i,:]})\n",
    "            #print(i)\n",
    "        use_time = time.time() - start_time\n",
    "        acc = sess.run(accuracy, feed_dict={images:vali_images, labels:vali_labels})\n",
    "        print(\"EPOCH {0}, training time {1:.4f}s, Validation Accuracy = {2:.3f}\".format(ep, use_time, acc))\n",
    "        # val_acc, _ = sess.run([accuracy, predictions],\n",
    "        #                        feed_dict={images: valid_images, labels: valid_labels})\n",
    "        # print(\"Validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
